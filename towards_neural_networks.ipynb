{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# to install pandas properly you might eed to use this for your virtual environment: sudo python3 -m pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Neural Networks\n",
    "\n",
    "This jupyter notebook inspired by the ML with Python course held by Peter Kocmann as part of the ProScience Computer Science Research coure at Technical University Berlin during Summer 2023. \n",
    "\n",
    "In this coure we went through very basic operations to understand Machine Learning bottom-up without uing the sophistcated libraries. Further literature that approaches the topic of Machine Learning in a similar way can be found here:\n",
    "\n",
    "Mathematische Algorithmen mit Python (V. Steinkamp, 2022)\n",
    "Neuronale Netze programmieren (J. Steinwendner, R. Schwaiger, 2020)\n",
    "\n",
    "The written and theoretical explanations are taken from Chat-GPT. \n",
    "The model we are working with is from the course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some theoretical backgrounds\n",
    "\n",
    "A while-program for adding two numbers is a computational algorithm that utilizes iterative loops and conditional statements to perform arithmetic operations. It follows a step-by-step procedure, updating variables and executing instructions until a specified condition is met. This type of algorithm is based on symbolic computation and explicit programming.\n",
    "\n",
    "On the other hand, a neural network is a computational model inspired by the structure and function of biological neural networks. It consists of interconnected artificial neurons or nodes organized in layers. Neural networks learn patterns and relationships from data through a training process, adjusting the weights and biases of the network connections to optimize its performance.\n",
    "\n",
    "The connection between a while-program and a neural network can be seen in the concept of universal approximation. It has been theoretically proven that certain types of neural networks, particularly those with a sufficient number of hidden layers and neurons, are capable of approximating any continuous function, including the addition operation performed by a while-program.\n",
    "\n",
    "In this sense, a neural network can be trained to approximate the behavior of a while-program for adding two numbers. By providing input values to the network and adjusting its parameters during training, the network can learn to produce outputs that approximate the desired sum of the two numbers.\n",
    "\n",
    "Although the implementation and underlying mechanisms of while-programs and neural networks differ significantly, their theoretical connection lies in the ability of neural networks to approximate the computational behavior of while-programs, including tasks like adding numbers, through their flexible and adaptive learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add-function without sum-operator in a simple while-programme\n",
    "def sum_wo_plus_operator(x, y):\n",
    "    counter = 0\n",
    "    while y != 0:\n",
    "        counter += 1\n",
    "        #print(counter,  'Calculation-step:')\n",
    "        #print('x is:', bin(x), 'y is:', bin(y))\n",
    "        #print('Compare all bits with each other, if both are 1 then carry_bit is 2, else the carry bit is 0.')\n",
    "        carry = x & y\n",
    "        #print('The carry bits result in:', bin(carry))\n",
    "        x = x ^ y\n",
    "        #print('x : ', x, '=', bin(x), '\\n')\n",
    "        y = carry << 1\n",
    "        #print('y : ', y, '=', bin(y), '\\n')\n",
    "    return x\n",
    "\n",
    "result = sum_wo_plus_operator(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a hard coded way to calculate the sum of two numbers following a \n",
    "# conceptional truthtable, in ML this is NOT what we want to do ;)\n",
    "\n",
    "def ai_sum(a, b):\n",
    "    if a == 0: return b\n",
    "    if b == 0: return a\n",
    "    if a == 1: \n",
    "        if b == 2:\n",
    "            return 2\n",
    "        elif b == 2:\n",
    "            return 3\n",
    "    elif a == 2:\n",
    "        if b == 1:\n",
    "            return 3\n",
    "        if b == 2:\n",
    "            return 4\n",
    "    elif a == 3:\n",
    "        if b == 1:\n",
    "            return 4\n",
    "        if b == 2:\n",
    "            return 5\n",
    "        if b == 3:\n",
    "            return 6\n",
    "\n",
    "ai_sum(2,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vector  Target\n",
      "[1.66, 1.56]       1\n",
      "  [2.0, 1.5]       0\n"
     ]
    }
   ],
   "source": [
    "# The model we are training has only two outcomes (binary clasification): 0 or 1\n",
    "# In real life the network would work with text, image or more complex numerical data\n",
    "\n",
    "data = {\"Input Vector\": [np.array([1.66, 1.56]), np.array([2, 1.5])], \"Target\": [1, 0]}\n",
    "model = pd.DataFrame(data)\n",
    "\n",
    "# printing the model without the indexes of the rows\n",
    "print(model.to_string(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards a neural network: some basic linear regression operations\n",
    "\n",
    "The following graph illustrates a simplified computation flow of a single-layer neural network. It demonstrates how inputs are combined with corresponding weights, passed through an activation function (sigmoid in this case), and produce a prediction. While this graph is a simplified representation, it captures the fundamental steps involved in neural network computations, such as weighted sums, activation functions, and output predictions.\n",
    "\n",
    "**Blue**: functions\n",
    "**Pink**: outputs.\n",
    "\n",
    "**Input**: The graph starts with an input node, which represents the input data to the neural network. It could be a single input or a vector of inputs.\n",
    "\n",
    "**Weights**: This node represents the parameters of the neural network. These weights determine the strength and impact of each input on the network's computation.\n",
    "\n",
    "**Dot Product and it's result**: This computes the dot product between the input data and the corresponding weights. This step multiplies each input with its respective weight and sums them up. The result of the dot product represents the weighted sum of the inputs, which is an intermediate computation in the neural network. The dot product calculation is a crucial operation in a neural network, it serves the *aggregation of inputs* and *linear tranformation*. #TODO: Link to an explanation of the two italic points.\n",
    "\n",
    "**Sum and Bias**: The bias term allows the neural network to introduce an offset or bias towards certain outputs.\n",
    "\n",
    "**Sigmoid**: This is the activation function, which is applied to the first layer.  The sigmoid function squashes the output into a range between 0 and 1, enabling non-linear transformations and introducing non-linearities into the neural network. It is a classification function, where 0 is 'no' and 1 is 'yes'.\n",
    "\n",
    "**Prediction**: The prediction can then be used for further explorations and calculation. \n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "\n",
    "classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff\n",
    "classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff\n",
    "\n",
    "id1.1[input] --> id2[dot_product]:::blue\n",
    "id1.2[weights] --> id2\n",
    "id4.1 --> id5(layer1_result):::pink\n",
    "\n",
    "\n",
    "    subgraph A\n",
    "        id2 --> id3(dot_product_result):::pink\n",
    "        id3 --> id4.1[sum]:::blue\n",
    "        id4.2(bias):::blue --> id4.1\n",
    "    end\n",
    "\n",
    "    subgraph B\n",
    "        id5 --> id6[sigmoid]:::blue\n",
    "        id6 --> id7(prediction):::pink\n",
    "    end\n",
    "```\n",
    "\n",
    "#TODO: Layer1 must actually be outside box a and b and in between those two\n",
    "\n",
    "#TODO: add explanation for second chart\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "\n",
    "id1(bias)\n",
    "id2(sum)\n",
    "id3(sigmoid)\n",
    "id4(error)\n",
    "\n",
    "id2 -- dlayer1_dbias --> id1\n",
    "id3 -- dprediction_dlayer1--> id2\n",
    "id4 -- derror_dprediction --> id3\n",
    "id4 -- derror_dbias --> id1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Functions\n",
    "######################################\n",
    "\n",
    "def calculate_dot_product(v1, v2, result=0):\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError('The vectors must be of same length!')\n",
    "    \n",
    "    for i in range(len(v1)):\n",
    "        result += v1[i] * v2[i]\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_dot_product_numpy(v1, v2, result=[]):\n",
    "    result = np.dot(v1, v2)\n",
    "    result = np.asarray(result)\n",
    "    return result\n",
    "\n",
    "def calculate_first_layer(x, y, w_0, w_1, bias):\n",
    "     temp0 = x * w_0\n",
    "     temp1 = y * w_1\n",
    "     first_layer = temp0 + temp1 + bias\n",
    "     return first_layer\n",
    "\n",
    "# sigmoid function results always between 0 and 1\n",
    "def sigmoid(x):\n",
    "     return 1/(1+np.exp(-1))\n",
    "\n",
    "def predict(input_vector, weight, bias):\n",
    "     # the following can also be written as np.dot()\n",
    "    temp_1 = v[0] * weight[0]\n",
    "    temp_2 = v[1] * weight[1]\n",
    "\n",
    "    layer_1 = temp_1 + temp_2 + bias\n",
    "    prediction = sigmoid(layer_1)\n",
    "    return prediction\n",
    "\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    return np.square(np.subtract(Y_true,Y_pred)).mean()\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations for our neural network build\n",
    "v = [1.72, 1.23]\n",
    "w = np.array([1.45, -0.66])\n",
    "b = 0.0\n",
    "\n",
    "# Initializations\n",
    "# TODO: initialize properly\n",
    "target = 0\n",
    "Y_true = 0\n",
    "\n",
    "prediction = predict(v, w, b)\n",
    "first_layer = calculate_first_layer(v[0], v[1], w[0], w[1], b)\n",
    "\n",
    "# note, todo: put it at the right playe, first layer is the layer before the activation function\n",
    "# sigmoid function is for categorization, aying 0 (no), and 1 (yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backpropagation: \n",
    "    def __init__(self):\n",
    "        # initializing the values with None ensures that they are available \n",
    "        # as attributes on the Backpropagation class instance and can be \n",
    "        # assigned values as needed thoughout the execution \n",
    "        self.weights = None\n",
    "        self.input_data = None\n",
    "        self.target = None\n",
    "        self.prediction = None\n",
    "        self.first_layer = None\n",
    "        self.bias = b\n",
    "    \n",
    "    # perform the forward pass in a neural network\n",
    "    # it takes input_data and weight to calculate the prediction\n",
    "    def forward_pass(self, input_data, weights, bias): \n",
    "        self.input_data = input_data \n",
    "        self.weights = weights\n",
    "        self.first_layer = self.calculate_first_layer()\n",
    "        self.prediction = self.sigmoid_1(self.first_layer)\n",
    "    \n",
    "    def calculate_first_layer(self):\n",
    "        return calculate_first_layer\n",
    "    \n",
    "    def sigmoid_1(self, x):\n",
    "        return sigmoid(x)\n",
    "    \n",
    "    def predict(self, input_vector, weight, bias):\n",
    "        return predict(input_vector, weight, bias)\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        return sigmoid_deriv(x)\n",
    "\n",
    "    def calculate_error_gradient(self, target):\n",
    "        self.target = target\n",
    "        derror_dprediction = 2 * (self.prediction - self.target)\n",
    "        dprediction_dlayer1 = self.sigmoid_deriv(self.first_layer)\n",
    "        dlayer1_dbias = 1  # Derivative of sum with respect to bias\n",
    "\n",
    "        derror_dbias = derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "\n",
    "        # Update bias\n",
    "        learning_rate = 0.1\n",
    "        self.bias -= learning_rate * derror_dbias\n",
    "\n",
    "        # Update weights\n",
    "        derror_dweights = derror_dprediction * dprediction_dlayer1 * self.input_data\n",
    "        self.weights -= learning_rate * derror_dweights\n",
    "\n",
    "        return derror_dprediction, self.weights, self.bias\n",
    "\n",
    "\n",
    "# Example usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [ 1.40055521 -0.69535877]\n",
      "Updated bias: -0.028746968091443028\n",
      "Error gradient: 1.4621171572600098\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_data = np.asarray(v)\n",
    "weights = w\n",
    "bias = 0.0\n",
    "\n",
    "backprop = Backpropagation()\n",
    "backprop.target = 0\n",
    "backprop.forward_pass(input_data, weights, bias)\n",
    "error_gradient, updated_weights, updated_bias = backprop.calculate_error_gradient(target=0)\n",
    "\n",
    "print(\"Updated weights:\", updated_weights)\n",
    "print(\"Updated bias:\", updated_bias)\n",
    "print(\"Error gradient:\", error_gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
